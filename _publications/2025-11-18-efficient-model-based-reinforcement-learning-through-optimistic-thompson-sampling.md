---
layout: publication-single
title: Efficient Model-Based Reinforcement Learning Through Optimistic Thompson
  Sampling
abstract: "Learning complex robot behavior through interactions with the
  environment necessitates principled exploration. Effective strategies should
  prioritize exploring regions of the state-action space that maximize rewards,
  with optimistic exploration emerging as a promising direction aligned with
  this idea and enabling sample-efficient reinforcement learning. However,
  existing methods overlook a crucial aspect: the need for optimism to be
  informed by a belief connecting the reward and state. To address this, we
  propose a practical, theoretically grounded approach to optimistic exploration
  based on Thompson sampling. Our model structure is the first that allows for
  reasoning about joint uncertainty over transitions and rewards. We apply our
  method on a set of MuJoCo and VMAS continuous control tasks. Our experiments
  demonstrate that optimistic exploration significantly accelerates learning in
  environments with sparse rewards, action penalties, and difficult-to-explore
  regions. Furthermore, we provide insights into when optimism is beneficial and
  emphasize the critical role of model uncertainty in guiding exploration."
published: 2025-11-18
authors:
  internal_authors:
    - Carl Henrik Ek
  external_authors:
    - family: Bayrooti
      given: "Jasmine "
    - family: Prorok
      given: "Amanda "
details:
  container-title: ICLR
  pdf: https://arxiv.org/pdf/2410.04988
  html: https://arxiv.org/abs/2410.04988
---
